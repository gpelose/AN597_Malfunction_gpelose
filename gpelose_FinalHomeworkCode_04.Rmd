---
title: "gpelose_FinalHomeworkCode_04"
author: "Greg Pelose"
date: "10/27/2019"
output: html_document
---

[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:

I am struggling on where to begin here, I will need to spend more time over tomorrow and sunday to try and figure this out.. 

```{r}
#first the one sample- normality guidelines:
z.test <- function(p1,n1,p0,p2=NULL,n2=NULL,conf.level=0.95,alternative="two.sided") {
   if(p1 == 0){
    return(0)}
    else {if((n1 * p1 < 5) | (n1 * (1-p1) > 5)){
          return(c("Warning: Not Normal", (p1-p0) / sqrt((p0 * (1-p0))/n1)))}
          else(return((p1-p0) / sqrt((p0 * (1-p0))/n1)))}
}
#now a function for two sample test
z.test.2 <- function(p1,n1,p0,p2,n2,conf.level=0.95,alternative="two.sided") {
   if(p1 == 0){
    return(0)}
    else {if((n1 * p1 < 5) | (n1 * (1-p1) > 5)){
          return(c("Warning: Not Normal", (p1-p0) / sqrt((p0 * (1-p0))/n1)))}
          else(return((p1-p0) / sqrt((p0 * (1-p0))/n1)))}
  #for second sample
  if(p2 == 0){
    return(0)}
    else {if((n2 * p2 < 5) | (n2 * (1-p2) > 5)){
          return(c("Warning: Not Normal", (p2-p0) / sqrt((p0 * (1-p0))/n2)))}
          else(return((p2-p0) / sqrt((p0 * (1-p0))/n2)))}
}
```

This is code from my lab member that I have been attempting to unpackage as I was unsuccessful in figuring this out on my own.  

[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both  longevity~brain size and log(longevity)~log(brain size):

I will first install my packages.

```{r}
install.packages("ggplot2")
install.packages("curl")
```

I will then load these packages into my library

```{r}
library(ggplot2)
library(curl)
```

Now I will be loading my dataset

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```

Success, remind to self to watch out for the NA values. 

```{r}
names(d)
```

```{r}
p <- plot(data = d, Brain_Size_Species_Mean ~ MaxLongevity_m, col = "blue", main = "Brain Size against Longevity")
geom_line(p)
```

creating a Model 1 regression.

```{r}
m <- lm(Brain_Size_Species_Mean ~ MaxLongevity_m, data = d)
m
summary(m)
```

I got an adjusted R-squared number of 0.4887, this shows me that 48 percent of the variation in y is explained by x. This is not statistically significant but this does show a slight significance. 

```{r}
plot(m)
```

```{r}
g <- ggplot(data = d, aes(x = MaxLongevity_m, y = Brain_Size_Species_Mean, main = "Brain Size vs. Longevity"))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g
```

The plot(m) graphs all look quite disorganized, by the scatterplot that I created in ggplot looks slightly normal in the beginning but has many more outliers as you increase both brian size and 
***
Next I will be taking the log of each of the variables to try and make the data come out better. 

```{r}
d$logMaxLongevity_m <- log(d$MaxLongevity_m)
d$logBrain_Size_Species_Mean <- log(d$Brain_Size_Species_Mean)
plot(data = d, logMaxLongevity_m ~ logBrain_Size_Species_Mean)
model <- lm(data = d, logMaxLongevity_m ~ logBrain_Size_Species_Mean)
summary(model)
plot(model)
```

Above, I added all of the code into one line of code. The r squared is .5751, this is a better number than before. 

```{r}
qqPlot(model$residuals)
```

The qqplot shows that everything is inside of the confidence intervals.
***
NEXT SECTION
***
Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0.

```{r}
b <- d$logBrain_Size_Species_Mean
l <- d$logMaxLongevity_m
slope <- lm(l ~ b)
slope
```

This function above shows me the slope (B1), this value is .2341.

```{r}
m <- lm(data = d, logMaxLongevity_m ~ logBrain_Size_Species_Mean)
ci <- confint(m, level = 0.90)  # using the results of lm()
ci

```

Now I will produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

```{r}
nci <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.95)  # for a vector of values
head(nci)
```

this above is my first attempt at a percent estimate.
I will try again.

```{r}
nci <- predict(m, newdata = data.frame(logBrain_Size_Species_Mean = 800), interval = "confidence", 
    level = 0.95)  
nci

```

Boom. Now I will try and graph this action.

Just kidding, I messed up and now I am going to try again. 

```{r}
nnci <- predict(m, newdata = data.frame(logBrain_Size_Species_Mean = d$logBrain_Size_Species_Mean), interval = "confidence", 
    level = 0.95)  # for a vector of values
head(nnci)
```

```{r}
df <- cbind(df, nnci)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr")
head(df)
```

```{r}
g <- ggplot(data = df, aes(x = x, y = y))
g <- g + geom_point(alpha = 1/2)
g <- g + geom_line(aes(x = x, y = CIfit), colour = "black")
g <- g + geom_line(aes(x = x, y = CIlwr), colour = "blue")
g <- g + geom_line(aes(x = x, y = CIupr), colour = "blue")
g
```

When comparing my plot from line 152 and my plot from 87 I see that in my qqplot from 87, many more values fall in the confidence intervals so I would say that this plot is better in my opinion. I am not exactly sure if I have analyzed this properly but this is what I got from my graphs. 

Challenges
1. My biggest challenge was figuring out number one. I tried to figure something out but really struggled with this. My group members code was able to help me kind of understand this but I am still quite lost with how they created their function. 
2. For the second questions I am able to find the code but I struggled to understand what the values fully meant for some of the poritons.
3. Time, I am struggling to find enough time, I need large chunks of time (4+) hours to really figure stuff out. I have been able to find many of these chunks but it is just not enough unfortunately. 
4. I need to re-go through some of the models that explain the statistics I think to fully understand what the meaning of some of these modules are telling me. 
5. I also learned that I need to relax a little more while doing this code. I struggled with relaxing when I feel like I cant figure stuff out but that is not good for me so I need to relax more. 





